[
  {
    "nodes": [
      0
    ],
    "node_names": [
      "nn_matmul_0"
    ],
    "group_id": 0,
    "input_desc": [
      [
        0,
        0
      ],
      [
        0,
        1
      ]
    ],
    "output_desc": [
      [
        0,
        0
      ]
    ],
    "code": "__global__ void __launch_bounds__(128) Group0(half* __restrict__ p0, half* __restrict__ p1, half* __restrict__ T_matmul_NN) {\n  int __bid = blockIdx.x;\n  const dim3 blockIdx(rasterization2DRow<32, 32, 12>(__bid), 0, 0);\n    __shared__ half p0_shared[8192];\n  __shared__ half p1_shared[8192];\n  ALLOCATE_CUTLASS_OBJECT(T_matmul_NN_cutlass_warp_mma, cutlass::gemm::warp::GemmTensorOp<\n    cutlass::gemm::GemmShape<64, 64, 32>,\n    cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<16, 32>,\n    cutlass::layout::RowMajorTensorOpMultiplicandCongruous<16, 64>\n>((((int)threadIdx.y) >> 1), (((int)threadIdx.y) & 1), ((int)threadIdx.x)));\n  #pragma unroll\n  for (int ax0_ax1_fused_0_0_0 = 0; ax0_ax1_fused_0_0_0 < 4; ++ax0_ax1_fused_0_0_0) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(p0_shared + (((((ax0_ax1_fused_0_0_0 * 1024) + (((int)threadIdx.y) * 256)) + ((((int)threadIdx.x) >> 2) * 32)) + ((((((int)threadIdx.x) >> 4) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + (((((((int)threadIdx.x) & 15) >> 3) + (((int)threadIdx.x) & 1)) & 1) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(p0 + ((((((((int)blockIdx.x) >> 5) * 524288) + (ax0_ax1_fused_0_0_0 * 131072)) + (((int)threadIdx.y) * 32768)) + ((((int)threadIdx.x) >> 2) * 4096)) + ((((int)threadIdx.x) & 3) * 8)))), \"n\"(16)\n    );\n  }\n  }\n  #pragma unroll\n  for (int ax0_ax1_fused_0_0_0_1 = 0; ax0_ax1_fused_0_0_0_1 < 4; ++ax0_ax1_fused_0_0_0_1) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(p1_shared + ((((((ax0_ax1_fused_0_0_0_1 * 1024) + (((int)threadIdx.y) * 256)) + ((((int)threadIdx.x) >> 3) * 64)) + (((((((int)threadIdx.x) & 7) >> 2) + (((int)threadIdx.y) >> 1)) & 1) * 32)) + (((((((int)threadIdx.x) & 3) >> 1) + (((int)threadIdx.y) & 1)) & 1) * 16)) + ((((((int)threadIdx.x) >> 4) + (((int)threadIdx.x) & 1)) & 1) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(p1 + (((((ax0_ax1_fused_0_0_0_1 * 32768) + (((int)threadIdx.y) * 8192)) + ((((int)threadIdx.x) >> 4) * 4096)) + ((((int)blockIdx.x) & 31) * 128)) + ((((int)threadIdx.x) & 15) * 8)))), \"n\"(16)\n    );\n  }\n  }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n  #pragma unroll\n  for (int ax0_ax1_fused_0_0_0_2 = 0; ax0_ax1_fused_0_0_0_2 < 4; ++ax0_ax1_fused_0_0_0_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(p0_shared + ((((((ax0_ax1_fused_0_0_0_2 * 1024) + (((int)threadIdx.y) * 256)) + ((((int)threadIdx.x) >> 2) * 32)) + ((((((int)threadIdx.x) >> 4) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + (((((((int)threadIdx.x) & 15) >> 3) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 4096)))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(p0 + (((((((((int)blockIdx.x) >> 5) * 524288) + (ax0_ax1_fused_0_0_0_2 * 131072)) + (((int)threadIdx.y) * 32768)) + ((((int)threadIdx.x) >> 2) * 4096)) + ((((int)threadIdx.x) & 3) * 8)) + 32))), \"n\"(16)\n    );\n  }\n  }\n  #pragma unroll\n  for (int ax0_ax1_fused_0_0_0_3 = 0; ax0_ax1_fused_0_0_0_3 < 4; ++ax0_ax1_fused_0_0_0_3) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(p1_shared + (((((((ax0_ax1_fused_0_0_0_3 * 1024) + (((int)threadIdx.y) * 256)) + ((((int)threadIdx.x) >> 3) * 64)) + (((((((int)threadIdx.x) & 7) >> 2) + (((int)threadIdx.y) >> 1)) & 1) * 32)) + (((((((int)threadIdx.x) & 3) >> 1) + (((int)threadIdx.y) & 1)) & 1) * 16)) + ((((((int)threadIdx.x) >> 4) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 4096)))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(p1 + ((((((ax0_ax1_fused_0_0_0_3 * 32768) + (((int)threadIdx.y) * 8192)) + ((((int)threadIdx.x) >> 4) * 4096)) + ((((int)blockIdx.x) & 31) * 128)) + ((((int)threadIdx.x) & 15) * 8)) + 131072))), \"n\"(16)\n    );\n  }\n  }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n__asm__ __volatile__(\"cp.async.wait_group 1;\");\n\n  __syncthreads();\n  call_cutlass_mma_prologue(T_matmul_NN_cutlass_warp_mma, (&(p0_shared[0])), (&(p1_shared[0])), 32, 128);\n  call_cutlass_mma_body(T_matmul_NN_cutlass_warp_mma);\n  for (int k_0 = 0; k_0 < 126; ++k_0) {\n    #pragma unroll\n    for (int ax0_ax1_fused_0_0_0_4 = 0; ax0_ax1_fused_0_0_0_4 < 4; ++ax0_ax1_fused_0_0_0_4) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(p0_shared + (((((((k_0 & 1) * 4096) + (ax0_ax1_fused_0_0_0_4 * 1024)) + (((int)threadIdx.y) * 256)) + ((((int)threadIdx.x) >> 2) * 32)) + ((((((int)threadIdx.x) >> 4) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + (((((((int)threadIdx.x) & 15) >> 3) + (((int)threadIdx.x) & 1)) & 1) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(p0 + ((((((((((int)blockIdx.x) >> 5) * 524288) + (ax0_ax1_fused_0_0_0_4 * 131072)) + (((int)threadIdx.y) * 32768)) + ((((int)threadIdx.x) >> 2) * 4096)) + (k_0 * 32)) + ((((int)threadIdx.x) & 3) * 8)) + 64))), \"n\"(16)\n    );\n  }\n    }\n    #pragma unroll\n    for (int ax0_ax1_fused_0_0_0_5 = 0; ax0_ax1_fused_0_0_0_5 < 4; ++ax0_ax1_fused_0_0_0_5) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(p1_shared + ((((((((k_0 & 1) * 4096) + (ax0_ax1_fused_0_0_0_5 * 1024)) + (((int)threadIdx.y) * 256)) + ((((int)threadIdx.x) >> 3) * 64)) + (((((((int)threadIdx.x) & 7) >> 2) + (((int)threadIdx.y) >> 1)) & 1) * 32)) + (((((((int)threadIdx.x) & 3) >> 1) + (((int)threadIdx.y) & 1)) & 1) * 16)) + ((((((int)threadIdx.x) >> 4) + (((int)threadIdx.x) & 1)) & 1) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(p1 + (((((((k_0 * 131072) + (ax0_ax1_fused_0_0_0_5 * 32768)) + (((int)threadIdx.y) * 8192)) + ((((int)threadIdx.x) >> 4) * 4096)) + ((((int)blockIdx.x) & 31) * 128)) + ((((int)threadIdx.x) & 15) * 8)) + 262144))), \"n\"(16)\n    );\n  }\n    }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n__asm__ __volatile__(\"cp.async.wait_group 1;\");\n\n    __syncthreads();\n    call_cutlass_mma_prologue(T_matmul_NN_cutlass_warp_mma, (&(p0_shared[(((k_0 + 1) & 1) * 4096)])), (&(p1_shared[(((k_0 + 1) & 1) * 4096)])), 32, 128);\n    call_cutlass_mma_epilogue(T_matmul_NN_cutlass_warp_mma);\n    call_cutlass_mma_body(T_matmul_NN_cutlass_warp_mma);\n  }\n__asm__ __volatile__(\"cp.async.wait_group 0;\");\n\n  __syncthreads();\n  call_cutlass_mma_prologue(T_matmul_NN_cutlass_warp_mma, (&(p0_shared[4096])), (&(p1_shared[4096])), 32, 128);\n  call_cutlass_mma_epilogue(T_matmul_NN_cutlass_warp_mma);\n  call_cutlass_mma_body(T_matmul_NN_cutlass_warp_mma);\n  call_cutlass_mma_epilogue(T_matmul_NN_cutlass_warp_mma);\n  #pragma unroll\n  for (int ax1_0 = 0; ax1_0 < 64; ++ax1_0) {\n    *(uint1*)(T_matmul_NN + (((((((((((int)blockIdx.x) >> 5) * 524288) + ((((int)threadIdx.y) >> 1) * 262144)) + ((ax1_0 & 7) * 32768)) + ((((int)threadIdx.x) >> 2) * 4096)) + ((((int)blockIdx.x) & 31) * 128)) + ((((int)threadIdx.y) & 1) * 64)) + ((ax1_0 >> 3) * 8)) + ((((int)threadIdx.x) & 3) * 2))) = *(uint1*)(T_matmul_NN_cutlass_warp_mma + (ax1_0 * 2));\n  }\n}\n\n",
    "block_size": [
      32,
      4,
      1
    ],
    "grid_size": [
      1024,
      1,
      1
    ],
    "latency": 1.0570216178894043,
    "name": "Group0",
    "gain": 0
  }
]